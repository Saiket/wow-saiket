--[[ DbcCSV by Saiket
DbcCSV.lua: Parses a CSV file into a table.

How to use:
  DbcCSV.ParseLine( Line ) - Parses a single CSV line into an array of string fields.
	DbcCSV.Parse( Filename, Key, ... ) - Reads a CSV file and packs its contents into
    a table.  The first row must contain type info generated by DBCUtil.exe.  An
    optional Key specifies which column to index the data by; Otherwise, rows
    are stored in the order they're read.  A list of optional names specify
    table keys to use rather than numerical column indices.  A name of false
    omits that column.
]]


local io = io;
local assert = assert;
local ipairs = ipairs;
local select = select;
local tonumber = tonumber;
local setmetatable = setmetatable;

module( "DbcCSV" );


-- DbcCSV.ParseLine: Converts a CSV line to a row table.  Source: <http://www.lua.org/pil/20.4.html>
function ParseLine ( Line )
	Line = Line..",";
	local Row, FieldStart = {}, 1;

	repeat
		if ( Line:match( '^"', FieldStart ) ) then -- Field starts with a '"'
			local Index, Quotes = FieldStart + 1;

			-- Scan until an unescaped closing quote is found
			repeat
				Quotes, Index = Line:match( '("+)()', Index );
				if ( not Index ) then -- End of line
					return nil, "Unmatched field quote.";
				end
			until ( #Quotes % 2 == 1 ) -- Last quote is unescaped

			Row[ #Row + 1 ] = Line:sub( FieldStart + 1, Index - 2 ):gsub( '""', '"' ); -- Unescape any enclosed quotes
			FieldStart = Line:find( ",", Index ) + 1;
		else -- Unquoted field
			local FieldNext = Line:find( ",", FieldStart );
			Row[ #Row + 1 ] = Line:sub( FieldStart, FieldNext - 1 );
			FieldStart = FieldNext + 1;
		end
	until ( FieldStart > #Line )

	Row[ #Row ] = nil; -- Remove added placeholder field
	return Row;
end


-- DbcCSV.Parse: Reads a CSV file and packs its contents into a table.  The
--   first row is used as type info for interpreting the rest of the file.
--   Names for columns can be provided to use as value keys rather than
--   numerical indices.  Logically false column names omit those columns.
--   Passing (true) as a column name uses that column's numeric index.  An
--   optional Key specifies which column to index rows by.  If no key column is
--   set, rows are inserted into the data table in the order they appear.
local function ConvertType ( Value, Type ) -- Cast data types into equivalent Lua data types
	if ( Type == "bool" ) then
		return Value == "1";
	elseif ( Type == "long" or Type == "float" or Type == "byte" ) then
		return tonumber( Value );
	else -- "flags" or "str" are kept as string data
		return Value;
	end
end
function Parse ( Filename, Key, ... )
	if ( Key ) then
		Key = assert( tonumber( Key ), "Key must be a column nunber." );
	end
	local Data = {};

	for Line in io.lines( Filename ) do
		local Row = assert( ParseLine( Line ) );

		if ( not Data.Types ) then -- Type data not yet parsed
			if ( Key and Row[ Key ] == "None" ) then
				error( "Key column has no data type." );
			end
			-- Put types in a meta-index so pairs won't catch it
			setmetatable( Data, { __index = {
				Types = Row;
			} } );
		else
			if ( not Key ) then -- Save rows in the order they're read
				Data[ #Data + 1 ] = Row;
			end

			for Index, Type in ipairs( Data.Types ) do
				if ( Type == "None" ) then -- Remove unused column
					Row[ Index ] = nil;
				else
					if ( Index == Key ) then -- Use this field to index the row
						Data[ ConvertType( Row[ Index ], Type ) ] = Row;
					end

					local Name = select( Index, ... );
					if ( not Name ) then -- Remove excluded column
						Row[ Index ] = nil;
					else -- Use field
						local Field = ConvertType( Row[ Index ], Type );

						-- Move named fields
						if ( Name == true ) then -- Use order rows are read in
							Row[ Index ] = Field; -- Update field data type
						else
							Row[ Index ] = nil; -- Remove original column
							Row[ Name ] = Field;
						end
					end
				end
			end
		end
	end

	return Data;
end
